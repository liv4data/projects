# -*- coding: utf-8 -*-
"""RecEngine1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bUDhTpfsKZvP3gfl1XcOD-Z1bKsu1QZA
"""

'''
Rec Engine
Notebook #1

This data creates a json file and a skip log.
It uses the CKAN API to obtain datasets and associated metadata from public datasets shared on data.gov.
The API maxes at 1000 requests, so batches are used to obtain the metadata.
Each dataset and its metadata are then appended to a parquet file for further data ingestion prior to cleaning.

As over 300,000 datasets are available on data.gov, this notebook was written and executed using pyspark in Databricks.
'''

#import necessary libraries
from pyspark.sql import SparkSession, Row
import requests, time, os, json
import pandas as pd
import numpy as np

#create and load progress file
progress_file = '/dbfs/user/hive/warehouse/datagov_progress.json'

#ensure directory exists before writing
os.makedirs(os.path.dirname(progress_file), exist_ok=True)

#try loading progress
if os.path.exists(progress_file):
    with open(progress_file, 'r') as f:
        progress = json.load(f)
        start_offset = progress.get('start', 0)
        notpublic_sets = progress.get('notpublic_sets', [])

#otherwise initialize variables for progress
else:
    start_offset = 0
    notpublic_sets = []

print(start_offset)

#create and load skip log
skip_log = '/dbfs/user/hive/warehouse/datagov_skips.csv'

#ensure directory exsits before writing
os.makedirs(os.path.dirname(skip_log), exist_ok=True)

#create file with headers if it doesn't exist
if not os.path.exists(skip_log):
    with open(skip_log, 'w') as f:
        f.write('start,stop\n')

skipped_batches = pd.read_csv(skip_log)

print(skipped_batches.loc[27:])

#define API base URL and parameters
base_url = 'https://catalog.data.gov/api/3/action/package_search'
batch_size = 1000
max_results = 313767

#create spark session
spark = SparkSession.builder.appName('DataGov Open Data Collection').getOrCreate()

#initialize skip list for datasets that are non-public
#notpublic_sets = []

#run smaller batch for each "non-inferred" batch
#could not infer: 111000, 117000, 118000, 302000, 305000, 306000, 307000, 308000, 309000
# 67300-67400, 99600-99700, 138000-138100, 176000-176100, 216100-216200, 216400-216500

batch_size = 100
start_offset = 305200
max_results = 306000

print(max_results)

def fetch_batch(start, retries=3, backoff=5):
    '''
    This function is used to determine the number of retries on a batch pull from data.gov.

    Args: start - integer indicating start of batch (i.e. the number of a specific dataset based on pagination on the website)
    '''
    for attempt in range(retries):
        try:
            response = requests.get(
                base_url,

                #parameters for the dataset pull
                params={
                    #'q': 'accessLevel:public',
                    'start':start, 'rows':batch_size},
                timeout=60
            )
            response.raise_for_status()
            return response.json()['result']['results']
        except Exception as e:
            print(f'Attempt {attempt + 1} failed at start={start}: {e}')
            time.sleep(backoff * (attempt+1))
    return None

#loop to pull information
for start in range(start_offset, max_results, batch_size):
    print(f'\nProcessing start={start}...')

    results = fetch_batch(start)
    if results is None:
        print(f'Skipping batch after multiple failed attempts.')
        #calculate stopping point of failed batch
        stop = start + batch_size

        #append failed batch info to log
        with open(skip_log, 'a') as f:
            f.write(f'{start},{stop}\n')

        #alert that batch failed to load
        print(f'Logged skipped batch: start = {start}, stop = {stop}.')
        continue

    #initialize list that will be used to append records to main file
    cleaned_records = []

    for dataset in results:
        #start by checking access level, only want publicly available with no restrictions
        access_level = next((i['value'] for i in dataset.get('extras', []) if i['key'] == 'accessLevel'), None)

        if access_level != 'public':
            #append the name of non-public datasets to list
            notpublic_sets.append({
                'title': dataset.get('title', 'N/A'),
                'access': access_level
            })
            continue

        cleaned_records.append({
            #get title of dataset
            'title': dataset.get('title', 'N/A'),

            #get dataset id
            'id': dataset.get('id', 'N/A'),

            #get dataset name - this is different from (but similar to) its title
            'name': dataset.get('name', 'N/A'),

            #return the date the metadata was last modified
            'medatdata_modified':dataset.get('metadata_modified', 'N/A'),

            #get the description of the dataset
            'description': dataset.get('notes', 'N/A'),

            #get the license use type
            'use_type':dataset.get('license_id', 'N/A'),

            #date the dataset was published
            'date_published': next((i['value'] for i in dataset.get('extras', []) if i ['key'] == 'issued'), None),

            #date the dataset was last modified
            'date_modified': next((i['value'] for i in dataset.get('extras', []) if i['key'] == 'modified'), None),

            #return the landing page for dataset
            'landing_page': next((i['value'] for i in dataset.get('extras', []) if i['key'] == 'landingPage'), None),

            #return the access level obtained earlier
            'access': access_level,

            #get organization title that posted the dataset within the organization dictionary
            'organization': dataset.get('organization', {}).get('title', 'N/A'),

            #get the organization shortened name/acronym from the organization dictionary
            'org': dataset.get('organization', {}).get('name', 'N/A'),

            #return the group title associated with the dataset
            'group': ', '.join([group['title'] for group in dataset.get('groups', [])]),

            #get the name for each resource in the resource dictionary/list
            'resources': ', '.join([resource['name'] for resource in dataset.get('resources', [])]),

            #get the name of tags attributed to the dataset and store as a list within the dataframe
            'tags': [tag['name'] for tag in dataset.get('tags', [])],

            #retrieve the formats for all files linked to the dataset
            'formats': ', '.join(
                sorted(set(res.get('format', '').upper() for res in dataset.get('resources', [])))
            )
        })
    if cleaned_records:
        #create spark dataframe from the list and append to parquet file
        batch_df = spark.createDataFrame(cleaned_records)
        batch_df.write.mode('append').parquet('dbfs:/user/hive/warehouse/open_data.parquet')

    #save progress checkpoint
    with open (progress_file, 'w') as f:
        json.dump({'start': start + batch_size, 'notpublic_sets': notpublic_sets}, f)

    #output progress
    print(f'Saved batch starting at {start:<10} datasets added: {len(cleaned_records):<10} non-public datasets (total): {len(notpublic_sets):<10}')

    time.sleep(1)